{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130cc9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM,LlamaConfig\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from utils import model_to_path_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ee5752",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '../../generated'\n",
    "behavior_data_dir = '../../behavior_data'\n",
    "story = 'pieman'\n",
    "original_transcript_dir = os.path.join(behavior_data_dir,'transcripts','moth_stories')\n",
    "with open(os.path.join(original_transcript_dir,'%s.txt'%story),'r') as f:\n",
    "    original_txt = f.read()\n",
    "\n",
    "device = 'cuda'\n",
    "model_name = 'Llama3-8b-instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_to_path_dict[model_name]['hf_name'])\n",
    "model = LlamaForCausalLM.from_pretrained(model_to_path_dict[model_name]['hf_name'],attn_implementation=\"eager\",device_map='auto',torch_dtype = torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216cfa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare prompt\n",
    "system_prompt = \"You are a human with limited memory ability. You're going to listen to a story, and your task is to recall the story and summarize it in your own words in a verbal recording. Respond as if youâ€™re speaking out loud.\"\n",
    "user_prompt = \"Here's the story: %s\\nHere's your recall: \"%original_txt\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "tokenized_chat = tokenized_chat.to(device)\n",
    "recall_start_index = tokenized_chat.shape[1]-5\n",
    "\n",
    "# only apply the attention temperature manipulation from recall to the story, not to the system & user prompt\n",
    "# find where the story starts\n",
    "story_start_idx = -1\n",
    "for i,t in enumerate(tokenized_chat[0]):\n",
    "    txt = tokenizer.decode(t)\n",
    "    if ':' in txt:\n",
    "        story_start_idx = i+1\n",
    "        break\n",
    "assert story_start_idx>0 and story_start_idx<tokenized_chat.shape[1],'failed to find story start idx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9702f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% time\n",
    "# attention temperatures, 0 = unmodified, the larger the number, the higher the temperature, the more diffuse the attention\n",
    "scales = [0,0.00005,0.00007,0.0002,0.0005,0.001] \n",
    "n = 5 # generate 5 recalls per temp\n",
    "output_by_scale = {}\n",
    "for scale in tqdm(scales):\n",
    "    outputs = []\n",
    "    for i in range(n):\n",
    "        if scale != 0:\n",
    "            output = model.generate(tokenized_chat,attention_scale = scale,recall_start_index = recall_start_index,\n",
    "                                story_start_index=story_start_idx,max_new_tokens = 800,output_attentions=False,\n",
    "                                return_dict_in_generate=True,output_scores=False)\n",
    "        else:\n",
    "            output = model.generate(tokenized_chat,story_start_index=story_start_idx,max_new_tokens = 800,\n",
    "                                    output_attentions=False,return_dict_in_generate=True,output_scores=False)\n",
    "        sequence = output['sequences']\n",
    "        outputs.append(tokenizer.decode(sequence[0][tokenized_chat.shape[1]:]))\n",
    "    output_by_scale[scale] = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b6c3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
